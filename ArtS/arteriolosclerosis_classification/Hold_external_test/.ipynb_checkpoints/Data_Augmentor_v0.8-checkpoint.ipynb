{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHANGELOG\n",
    "\n",
    "v0.8 11/18/2023\n",
    "- hold out and external testing\n",
    "\n",
    "v0.7 10/15/2023\n",
    "- modified 0.6 for classification augmentation\n",
    "\n",
    "v0.6 10/10/2023\n",
    "- load each raw shard one at a time, augment, then save\n",
    "\n",
    "v0.5 10/9/2023\n",
    "- convert to Vessel_WallsLumen_Segmentation version\n",
    "- repeat 30 times (done for original dataset)\n",
    "\n",
    "v0.4\n",
    "- initiate version for arteriolosclerotic vessel classification and segmentation\n",
    "\n",
    "v0.3\n",
    "    augment = tf.keras.Sequential([\n",
    "        layers.RandomTranslation(0.08, 0.08, fill_mode='wrap'),\n",
    "        layers.RandomRotation(1, fill_mode='wrap'),\n",
    "        layers.RandomFlip(),\n",
    "    ])\n",
    "    \n",
    "    image = rgb_perturb_stain_concentration(image, sigma1=0.58, sigma2=0.27)\n",
    "\n",
    "v0.2\n",
    "- rolling color augmentation into tf.data.Dataset.map.. this seems to resolve memory issue\n",
    "\n",
    "v0.1 6/29/2023\n",
    "- add in color augmentation\n",
    "- add in restart run all function to refresh memory each loop\n",
    "\n",
    "v0.0 6/16/23:\n",
    "- no color augmentation\n",
    "- no stain normalization\n",
    "\n",
    "Pseudocode Outline\n",
    "\n",
    "Create list of folder names in wsi-arterio_batches_all/data\n",
    "Loop through list of folder names\n",
    "    Load train dats and msk\n",
    "    Load valid dats and msk\n",
    "    Augment train dats and msk tf dataset\n",
    "    Convert valid dats and msk to tf dataset\n",
    "    Save train and valid into the same folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[ 2024-04-02 22:54:31 ] CUDA_VISIBLE_DEVICES automatically set to: 1           \n"
     ]
    }
   ],
   "source": [
    "from jarvis.utils.general import gpus\n",
    "gpus.autoselect()\n",
    "\n",
    "import glob, numpy as np, pandas as pd, tensorflow as tf, matplotlib.pyplot as plt, skimage.io, os, time\n",
    "from tensorflow.keras import Input, Model, models, layers, optimizers, losses, callbacks, utils\n",
    "from pathlib import Path\n",
    "from jarvis.utils.display import imshow\n",
    "from PIL import Image\n",
    "from skimage.transform import rescale, resize\n",
    "from histomicstk.preprocessing.augmentation import rgb_perturb_stain_concentration\n",
    "from IPython.display import HTML, Javascript, display\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "import sys  \n",
    "sys.path.append('/home/jjlou/Jerry/jerry_packages')\n",
    "from jerry_utils import restart_kernel, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/jjlou/Jerry/wsi-arterio/arteriosclerotic_vessel_detection_and_fine_segmentation/Arteriolosclerosis_classification/data_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random flip, translation, and rotation via tensorflow layers\n",
    "def aug(images, labels):\n",
    "    \n",
    "    augment = tf.keras.Sequential([\n",
    "        layers.RandomTranslation(0.05, 0.05),\n",
    "        layers.RandomRotation(1),\n",
    "        layers.RandomFlip(),\n",
    "    ])\n",
    "       \n",
    "    images = tf.cast(images, 'uint8')\n",
    "      \n",
    "    images = augment(images)  \n",
    "    \n",
    "    return tf.cast(images, 'uint8'), tf.cast(labels, 'uint8') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_aug(image, labels): \n",
    "    image = rgb_perturb_stain_concentration(image, sigma1=0.43, sigma2=0.17)\n",
    "    return tf.cast(image, 'uint8'), tf.cast(labels, 'uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.numpy_function creates output with unknown shape.. so need to reshape it\n",
    "def reshape(image, labels):\n",
    "    image = tf.reshape(image, [512,512,3])\n",
    "    labels = tf.reshape(labels, [1,])\n",
    "    return tf.cast(image, 'uint8'), tf.cast(labels, 'uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_save = f'{root}/Train_Color.BasicMorph.Aug_x38'\n",
    "train_shards_load_pos = sorted(glob.glob(f'{root}/train/pos/*'))    \n",
    "train_shards_load_neg = sorted(glob.glob(f'{root}/train/neg/*'))\n",
    "\n",
    "train_shard_num_pos = len(train_shards_load_pos)\n",
    "train_shard_num_neg = len(train_shards_load_neg)\n",
    "\n",
    "# Augment Train Pos\n",
    "# repeat 19 times since average pos/neg patch ratio is 19 and we well repeat neg shards x2\n",
    "if not glob.glob(f'{train_save}/pos_shard_{train_shard_num_pos-1}/*/*/*.snapshot'):\n",
    "    for s in train_shards_load_pos:\n",
    "        shard_save = f'{train_save}/pos_shard_{train_shards_load_pos.index(s)}'\n",
    "        if not glob.glob(f'{shard_save}/*/*/*.snapshot'):\n",
    "            assert not glob.glob(f'{shard_save}/*/*.shard'), f'{shard_save} did not save properly'\n",
    "            shard = tf.data.Dataset.load(s)\n",
    "            shard = (\n",
    "                shard.shuffle(len(shard), reshuffle_each_iteration=True)\n",
    "                .repeat(38)\n",
    "                .map(lambda i,m: tf.numpy_function(color_aug, [i,m], [tf.uint8, tf.uint8]), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                .map(lambda i,m: reshape(i,m), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                .map(lambda i,m: aug(i,m), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                .batch(1)\n",
    "            ) \n",
    "            shard.save(shard_save)\n",
    "            restart_kernel()\n",
    "            time.sleep(10)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Augment Train Neg\n",
    "if not glob.glob(f'{train_save}/neg_shard_{train_shard_num_neg-1}/*/*/*.snapshot'):\n",
    "    for s in train_shards_load_neg:\n",
    "        shard_save = f'{train_save}/neg_shard_{train_shards_load_neg.index(s)}'\n",
    "        if not glob.glob(f'{shard_save}/*/*/*.snapshot'):\n",
    "            assert not glob.glob(f'{shard_save}/*/*.shard'), f'{shard_save} did not save properly'\n",
    "            shard = tf.data.Dataset.load(s)\n",
    "            shard = (\n",
    "                shard.shuffle(len(shard), reshuffle_each_iteration=True)\n",
    "                .repeat(2)\n",
    "                .map(lambda i,m: tf.numpy_function(color_aug, [i,m], [tf.uint8, tf.uint8]), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                .map(lambda i,m: reshape(i,m), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                .map(lambda i,m: aug(i,m), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                .batch(1)\n",
    "            ) \n",
    "            shard.save(shard_save)\n",
    "            restart_kernel()\n",
    "            time.sleep(10)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_save = f'{root}/hold_processed'\n",
    "hold_pos = sorted(glob.glob(f'{root}/hold_out/pos/*'))    \n",
    "hold_neg = sorted(glob.glob(f'{root}/hold_out/neg/*'))\n",
    "hold_load = hold_pos + hold_neg\n",
    "\n",
    "if len(glob.glob(f'{hold_save}/*')) < len(hold_load):\n",
    "    for s in hold_load:\n",
    "        modifier = s.split('/')[-2]\n",
    "        shard_save = f'{hold_save}/{modifier}_shard_{hold_load.index(s)}'\n",
    "        if not glob.glob(f'{shard_save}/*/*/*.snapshot'):\n",
    "            assert not glob.glob(f'{shard_save}/*/*.shard'), f'{shard_save} did not save properly'\n",
    "            shard = tf.data.Dataset.load(s)\n",
    "            shard = shard.batch(1)\n",
    "            shard.save(shard_save)\n",
    "            restart_kernel()\n",
    "            time.sleep(10)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_save = f'{root}/external_processed'\n",
    "external_pos = sorted(glob.glob(f'{root}/external/pos/*'))    \n",
    "external_neg = sorted(glob.glob(f'{root}/external/neg/*'))\n",
    "external_load = external_pos + external_neg\n",
    "\n",
    "if len(glob.glob(f'{external_save}/*')) < len(external_load):\n",
    "    for s in external_load:\n",
    "        modifier = s.split('/')[-2]\n",
    "        shard_save = f'{external_save}/{modifier}_shard_{external_load.index(s)}'\n",
    "        if not glob.glob(f'{shard_save}/*/*/*.snapshot'):\n",
    "            assert not glob.glob(f'{shard_save}/*/*.shard'), f'{shard_save} did not save properly'\n",
    "            shard = tf.data.Dataset.load(s)\n",
    "            shard = shard.batch(1)\n",
    "            shard.save(shard_save)\n",
    "            restart_kernel()\n",
    "            time.sleep(10)\n",
    "        else:\n",
    "            continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
