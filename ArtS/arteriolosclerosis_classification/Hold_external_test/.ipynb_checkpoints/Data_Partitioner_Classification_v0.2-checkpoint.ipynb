{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHANGELOG\n",
    "\n",
    "v0.2 11/18/2023\n",
    "- hold out and external testing\n",
    "\n",
    "v0.1 10/13/2023\n",
    "- change to classification version\n",
    "\n",
    "v0.0 7/14/2023\n",
    "- initiate from the wsi-arterio_batches_all_classification Data_Partitioner_v0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[ 2023-11-19 05:25:35 ] WARNING 1 GPU device(s) requested but only 0 available \n"
     ]
    }
   ],
   "source": [
    "from jarvis.utils.general import gpus\n",
    "gpus.autoselect()\n",
    "\n",
    "import glob, numpy as np, pandas as pd, tensorflow as tf, matplotlib.pyplot as plt, os, gc\n",
    "from jarvis.utils.display import imshow\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import after setting OPENCV_IO_MAX_IMAGE_PIXELS to 2^50\n",
    "os.environ[\"OPENCV_IO_MAX_IMAGE_PIXELS\"] = pow(2,50).__str__() \n",
    "import cv2\n",
    "\n",
    "import sys  \n",
    "sys.path.append('/home/jjlou/Jerry/jerry_packages')\n",
    "from jerry_utils import restart_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/jjlou/Jerry/wsi-arterio/arteriosclerotic_vessel_detection_and_fine_segmentation/Arteriolosclerosis_classification/data_test'\n",
    "shape = (512, 512) # resize all images to this shape\n",
    "shard_size = 5 #load this number of file addresses for each shard\n",
    "\n",
    "hold_out_list = ['UCI-37-18', 'D-492']\n",
    "external_list = ['V019-B1_VP', 'V019_B3_VP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of all patches (dats) and list of all masks (msk)\n",
    "dats_pos_batches1and2 = sorted(glob.glob('/data/raw/wsi_arterio_2nd_alg/data/pos/*/*'))\n",
    "dats_pos_batch3 = sorted(glob.glob('/data/raw/wsi_arterio_2nd_alg/batch_3_data/annotations/pos/*/*'))\n",
    "dats_pos = dats_pos_batches1and2 + dats_pos_batch3\n",
    "\n",
    "dats_neg_batches1and2 = sorted(glob.glob('/data/raw/wsi_arterio_2nd_alg/data/neg/*/*'))\n",
    "dats_neg_batch3 = sorted(glob.glob('/data/raw/wsi_arterio_2nd_alg/batch_3_data/annotations/neg/*/*'))\n",
    "dats_neg = dats_neg_batches1and2 + dats_neg_batch3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dats=None, cls=None, shape=None):\n",
    "    images = [tf.cast(cv2.resize(cv2.imread(d), shape), dtype='uint8') for d in dats]\n",
    "    labels = [tf.cast(cls, dtype='uint8') for _ in range(len(images))]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(dats=None, cls=None, shape=None, shard_size=shard_size, path_root=None, path_modifier=''):\n",
    "    # Only need a path_modifier if \"concatenating\" datasets by saving to the same path_root\n",
    "    iterate = [n for n in range(0, len(dats), shard_size)]\n",
    "    for i in iterate:\n",
    "        save_path = f'{path_root}/{path_modifier}shard_{iterate.index(i)}'\n",
    "        if not glob.glob(f'{save_path}/*/*/*.snapshot'):\n",
    "            assert not glob.glob(f'{save_path}/*/*.shard'), f'{save_path} did not save properly'\n",
    "            \n",
    "            shard_dats = dats[i:i+shard_size]\n",
    "            shard = load_data(dats=shard_dats, cls=cls, shape=shape)\n",
    "            shard.save(save_path) \n",
    "            \n",
    "            if i+shard_size-1 < len(dats):\n",
    "                record = {'start': dats[i], 'stop': dats[i+shard_size-1]}\n",
    "            else:\n",
    "                record = {'start': dats[i], 'stop': dats[len(dats)-1]}\n",
    "            record = pd.DataFrame.from_dict(record, orient='index')\n",
    "            record.to_csv(f'{save_path}/record.csv') \n",
    "            \n",
    "            restart_kernel()\n",
    "            time.sleep(10)\n",
    "\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_out_dats_pos = []\n",
    "hold_out_dats_neg = []\n",
    "\n",
    "# Find all instances of the hold out patient IDs in dats and msk then save in corresponding list\n",
    "for h in hold_out_list:\n",
    "    for d in dats_pos:\n",
    "        if d.find(h) != -1:\n",
    "            hold_out_dats_pos.append(d)\n",
    "        else:\n",
    "            continue \n",
    "    for dn in dats_neg:\n",
    "        if dn.find(h) != -1:\n",
    "            hold_out_dats_neg.append(dn)\n",
    "            \n",
    "hold_path = f'{root}/hold_out'   \n",
    "hold_num_pos = len([n for n in range(0, len(hold_out_dats_pos), shard_size)]) - 1\n",
    "hold_num_neg = len([n for n in range(0, len(hold_out_dats_neg), shard_size)]) - 1\n",
    "\n",
    "if not glob.glob(f'{hold_path}/pos/pos_shard_{hold_num_pos}/*/*/*.snapshot'):\n",
    "    \n",
    "    # process positive hold out\n",
    "    process_data(\n",
    "        dats=hold_out_dats_pos, \n",
    "        cls=1, \n",
    "        shape=shape, \n",
    "        shard_size=shard_size, \n",
    "        path_root=f'{hold_path}/pos', \n",
    "        path_modifier='pos_')\n",
    "\n",
    "if not glob.glob(f'{hold_path}/neg/neg_shard_{hold_num_neg}/*/*/*.snapshot'):\n",
    "    \n",
    "    # process negative hold out\n",
    "    process_data(\n",
    "        dats=hold_out_dats_neg, \n",
    "        cls=0, \n",
    "        shape=shape, \n",
    "        shard_size=shard_size, \n",
    "        path_root=f'{hold_path}/neg', \n",
    "        path_modifier='neg_')\n",
    "\n",
    "# Remove hold out dats and msks from original list\n",
    "for d in hold_out_dats_pos:\n",
    "    dats_pos.remove(d)\n",
    "for n in hold_out_dats_neg:\n",
    "    dats_neg.remove(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_dats_pos = []\n",
    "external_dats_neg = []\n",
    "\n",
    "# Find all instances of the external  patient IDs in dats and msk then save in corresponding list\n",
    "for h in external_list:\n",
    "    for d in dats_pos:\n",
    "        if d.find(h) != -1:\n",
    "            external_dats_pos.append(d)\n",
    "        else:\n",
    "            continue \n",
    "    for dn in dats_neg:\n",
    "        if dn.find(h) != -1:\n",
    "            external_dats_neg.append(dn)\n",
    "            \n",
    "external_path = f'{root}/external'   \n",
    "external_num_pos = len([n for n in range(0, len(external_dats_pos), shard_size)]) - 1\n",
    "external_num_neg = len([n for n in range(0, len(external_dats_neg), shard_size)]) - 1\n",
    "\n",
    "if not glob.glob(f'{external_path}/pos/pos_shard_{external_num_pos}/*/*/*.snapshot'):\n",
    "    \n",
    "    # process positive external \n",
    "    process_data(\n",
    "        dats=external_dats_pos, \n",
    "        cls=1, \n",
    "        shape=shape, \n",
    "        shard_size=shard_size, \n",
    "        path_root=f'{external_path}/pos', \n",
    "        path_modifier='pos_')\n",
    "\n",
    "if not glob.glob(f'{external_path}/neg/neg_shard_{external_num_neg}/*/*/*.snapshot'):\n",
    "    \n",
    "    # process negative external \n",
    "    process_data(\n",
    "        dats=external_dats_neg, \n",
    "        cls=0, \n",
    "        shape=shape, \n",
    "        shard_size=shard_size, \n",
    "        path_root=f'{external_path}/neg', \n",
    "        path_modifier='neg_')\n",
    "\n",
    "# Remove external  dats and msks from original list\n",
    "for d in external_dats_pos:\n",
    "    dats_pos.remove(d)\n",
    "for n in external_dats_neg:\n",
    "    dats_neg.remove(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = f'{root}/train'   \n",
    "train_num_pos = len([n for n in range(0, len(dats_pos), shard_size)]) - 1\n",
    "train_num_neg = len([n for n in range(0, len(dats_neg), shard_size)]) - 1\n",
    "\n",
    "if not glob.glob(f'{train_path}/pos/pos_shard_{train_num_pos}/*/*/*.snapshot'):\n",
    "    \n",
    "    # process positive train out\n",
    "    process_data(\n",
    "        dats=dats_pos, \n",
    "        cls=1, \n",
    "        shape=shape, \n",
    "        shard_size=shard_size, \n",
    "        path_root=f'{train_path}/pos', \n",
    "        path_modifier='pos_')\n",
    "\n",
    "if not glob.glob(f'{train_path}/neg/neg_shard_{train_num_neg}/*/*/*.snapshot'):\n",
    "    \n",
    "    # process negative train out\n",
    "    process_data(\n",
    "        dats=dats_neg, \n",
    "        cls=0, \n",
    "        shape=shape, \n",
    "        shard_size=shard_size, \n",
    "        path_root=f'{train_path}/neg', \n",
    "        path_modifier='neg_')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
