{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHANGELOG\n",
    "\n",
    "v0.8\n",
    "- hold out and external testing\n",
    "\n",
    "v0.7 11/1/2023\n",
    "- divide train into pos and neg masks\n",
    "- repeat pos x5 because there are about x5 neg compared to pos dats/mask pairs\n",
    "\n",
    "v0.6 10/9/2023\n",
    "- change to segmentation mask only\n",
    "- load each shard one by one, augment, then save\n",
    "\n",
    "v0.5\n",
    "- revise v0.4 to version for vessel detection and rough segmentation\n",
    "\n",
    "v0.4\n",
    "- initiate version for arteriolosclerotic vessel classification and fine segmentation\n",
    "\n",
    "v0.3\n",
    "    augment = tf.keras.Sequential([\n",
    "        layers.RandomTranslation(0.08, 0.08, fill_mode='wrap'),\n",
    "        layers.RandomRotation(1, fill_mode='wrap'),\n",
    "        layers.RandomFlip(),\n",
    "    ])\n",
    "    \n",
    "    image = rgb_perturb_stain_concentration(image, sigma1=0.58, sigma2=0.27)\n",
    "\n",
    "v0.2\n",
    "- rolling color augmentation into tf.data.Dataset.map.. this seems to resolve memory issue\n",
    "\n",
    "v0.1 6/29/2023\n",
    "- add in color augmentation\n",
    "- add in restart run all function to refresh memory each loop\n",
    "\n",
    "v0.0 6/16/23:\n",
    "- no color augmentation\n",
    "- no stain normalization\n",
    "\n",
    "Pseudocode Outline\n",
    "\n",
    "Create list of folder names in wsi-arterio_batches_all/data\n",
    "Loop through list of folder names\n",
    "    Load train dats and msk\n",
    "    Load valid dats and msk\n",
    "    Augment train dats and msk tf dataset\n",
    "    Convert valid dats and msk to tf dataset\n",
    "    Save train and valid into the same folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[ 2023-11-24 03:50:56 ] CUDA_VISIBLE_DEVICES automatically set to: 3           \n"
     ]
    }
   ],
   "source": [
    "from jarvis.utils.general import gpus\n",
    "gpus.autoselect()\n",
    "\n",
    "import glob, numpy as np, pandas as pd, tensorflow as tf, matplotlib.pyplot as plt, skimage.io, os, gc\n",
    "from tensorflow.keras import Input, Model, models, layers, optimizers, losses, callbacks, utils\n",
    "from pathlib import Path\n",
    "from jarvis.utils.display import imshow\n",
    "from PIL import Image\n",
    "from skimage.transform import rescale, resize\n",
    "from histomicstk.preprocessing.augmentation import rgb_perturb_stain_concentration\n",
    "from IPython.display import HTML, Javascript, display\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "import sys  \n",
    "sys.path.append('/home/jjlou/Jerry/jerry_packages')\n",
    "from jerry_utils import restart_kernel, load_dataset, save_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/jjlou/Jerry/wsi-arterio/vessel_detection_and_rough_segmentation/data_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random flip and random crop via tensorflow layers\n",
    "def aug(images, mask):\n",
    "    \n",
    "    augment = tf.keras.Sequential([\n",
    "        layers.RandomCrop(512, 512),\n",
    "        layers.RandomFlip()\n",
    "    ])\n",
    "       \n",
    "    images = tf.cast(images, 'uint8')\n",
    "    mask = tf.cast(mask, 'uint8') \n",
    "    \n",
    "    mask = tf.stack([mask, mask, mask], -1)\n",
    "    \n",
    "    images_mask = tf.concat([images, mask], -1)  \n",
    "    images_mask = augment(images_mask)  \n",
    "    \n",
    "    image = images_mask[:,:,:3]\n",
    "    mask = images_mask[:,:,3]\n",
    "    mask = tf.expand_dims(mask,axis=2)\n",
    "    \n",
    "    return tf.cast(image, 'uint8'), tf.cast(mask, 'uint8') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_aug(image, mask): \n",
    "    image = rgb_perturb_stain_concentration(image, sigma1=0.43, sigma2=0.17)\n",
    "    return tf.cast(image, 'uint8'), tf.cast(mask, 'uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.numpy_function creates output with unknown shape.. so need to reshape it\n",
    "def reshape(image, mask):\n",
    "    image = tf.reshape(image, [512,512,3])\n",
    "    mask = tf.reshape(mask, [512,512,1])\n",
    "    return tf.cast(image, 'uint8'), tf.cast(mask, 'uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_mask_dims(image, mask):\n",
    "    mask = tf.expand_dims(mask, axis=2)\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_train(load=None, save_path=None):\n",
    "    for s in load:\n",
    "        ID = s.split('/')[-2]\n",
    "        shard_num = s.split('/')[-1]\n",
    "        save_file = f'{save_path}/{ID}/{shard_num}'\n",
    "        if not glob.glob(f'{save_file}/*/*/*.snapshot'):\n",
    "            assert not glob.glob(f'{save_file}/*/*.shard'), f'{save_file} did not save'\n",
    "            shard = tf.data.Dataset.load(s)\n",
    "            shard = (\n",
    "                shard.map(lambda i, m: aug(i, m))\n",
    "                #.map(lambda i, m: tf.numpy_function(color_aug, [i, m], [tf.uint8, tf.uint8]))\n",
    "                #.map(lambda i, m: reshape(i, m))\n",
    "                .batch(1)\n",
    "            )\n",
    "            shard.save(save_file)\n",
    "            restart_kernel()\n",
    "            time.sleep(10)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_valid(load=None, save_path=None):\n",
    "    for s in load:\n",
    "        ID = s.split('/')[-2]\n",
    "        shard_num = s.split('/')[-1]\n",
    "        save_file = f'{save_path}/{ID}/{shard_num}'\n",
    "        if not glob.glob(f'{save_file}/*/*/*.snapshot'):\n",
    "            assert not glob.glob(f'{save_file}/*/*.shard'), f'{save_file} did not save'\n",
    "            shard = tf.data.Dataset.load(s)\n",
    "            shard = (\n",
    "                shard.map(lambda i,m: expand_mask_dims(i, m), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                .batch(1)\n",
    "            )\n",
    "            shard.save(save_file)\n",
    "            restart_kernel()\n",
    "            time.sleep(10)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_load = sorted(glob.glob(f'{root}/train_balanced/*/*'))\n",
    "train_save = f'{root}/train_Basic.Aug'\n",
    "\n",
    "process_data_train(load=train_load, save_path=train_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_load = sorted(glob.glob(f'{root}/external_filtered/*/*'))\n",
    "external_save = f'{root}/external_processed'\n",
    "\n",
    "process_data_valid(load=external_load, save_path=external_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_out_load = sorted(glob.glob(f'{root}/hold_out_filtered/*/*'))\n",
    "hold_out_save = f'{root}/hold_out_processed'\n",
    "\n",
    "process_data_valid(load=hold_out_load, save_path=hold_out_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
