{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHANGELOG\n",
    "\n",
    "v1.0\n",
    "- change back to sigma1=0.43, sigma2=0.17\n",
    "\n",
    "v0.9\n",
    "- save each repeat copy as a single fragment individually\n",
    "\n",
    "v0.8 11/13/2023\n",
    "- conver to train and hold out test\n",
    "\n",
    "v0.6 10/10/2023\n",
    "- load each raw shard one at a time, augment, then save\n",
    "\n",
    "v0.5 10/9/2023\n",
    "- convert to Vessel_WallsLumen_Segmentation version\n",
    "- repeat 30 times (done for original dataset)\n",
    "\n",
    "v0.4\n",
    "- initiate version for arteriolosclerotic vessel classification and segmentation\n",
    "\n",
    "v0.3\n",
    "    augment = tf.keras.Sequential([\n",
    "        layers.RandomTranslation(0.08, 0.08, fill_mode='wrap'),\n",
    "        layers.RandomRotation(1, fill_mode='wrap'),\n",
    "        layers.RandomFlip(),\n",
    "    ])\n",
    "    \n",
    "    image = rgb_perturb_stain_concentration(image, sigma1=0.58, sigma2=0.27)\n",
    "\n",
    "v0.2\n",
    "- rolling color augmentation into tf.data.Dataset.map.. this seems to resolve memory issue\n",
    "\n",
    "v0.1 6/29/2023\n",
    "- add in color augmentation\n",
    "- add in restart run all function to refresh memory each loop\n",
    "\n",
    "v0.0 6/16/23:\n",
    "- no color augmentation\n",
    "- no stain normalization\n",
    "\n",
    "Pseudocode Outline\n",
    "\n",
    "Create list of folder names in wsi-arterio_batches_all/data\n",
    "Loop through list of folder names\n",
    "    Load train dats and msk\n",
    "    Load valid dats and msk\n",
    "    Augment train dats and msk tf dataset\n",
    "    Convert valid dats and msk to tf dataset\n",
    "    Save train and valid into the same folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[ 2023-12-03 21:39:41 ] CUDA_VISIBLE_DEVICES automatically set to: 0           \n"
     ]
    }
   ],
   "source": [
    "from jarvis.utils.general import gpus\n",
    "gpus.autoselect()\n",
    "\n",
    "import glob, numpy as np, pandas as pd, tensorflow as tf, matplotlib.pyplot as plt, skimage.io, os, time\n",
    "from tensorflow.keras import Input, Model, models, layers, optimizers, losses, callbacks, utils\n",
    "from pathlib import Path\n",
    "from jarvis.utils.display import imshow\n",
    "from PIL import Image\n",
    "from skimage.transform import rescale, resize\n",
    "from histomicstk.preprocessing.augmentation import rgb_perturb_stain_concentration\n",
    "from IPython.display import HTML, Javascript, display\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "import sys  \n",
    "sys.path.append('/home/jjlou/Jerry/jerry_packages')\n",
    "from jerry_utils import restart_kernel, load_dataset, show_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/jjlou/Jerry/wsi-arterio/arteriosclerotic_vessel_detection_and_fine_segmentation/Vessel_WallsLumen_Segmentation/data_test_v2'\n",
    "repeats = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random flip via tensorflow layers\n",
    "def aug(images, mask):\n",
    "    \n",
    "    augment = tf.keras.Sequential([\n",
    "        layers.RandomTranslation(0.05, 0.05),\n",
    "        layers.RandomRotation(1),\n",
    "        layers.RandomFlip(),\n",
    "    ])\n",
    "       \n",
    "    images = tf.cast(images, 'uint8')\n",
    "    mask = tf.cast(mask, 'uint8') \n",
    "    \n",
    "    mask = tf.stack([mask, mask, mask], -1)\n",
    "    \n",
    "    images_mask = tf.concat([images, mask], -1)  \n",
    "    images_mask = augment(images_mask)  \n",
    "    \n",
    "    image = images_mask[:,:,:3]\n",
    "    mask = images_mask[:,:,3]\n",
    "    mask = tf.expand_dims(mask,axis=2)\n",
    "    \n",
    "    return tf.cast(image, 'uint8'), tf.cast(mask, 'uint8') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_aug(image, mask): \n",
    "    image = rgb_perturb_stain_concentration(image, sigma1=0.43, sigma2=0.17)\n",
    "    return tf.cast(image, 'uint8'), tf.cast(mask, 'uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.numpy_function creates output with unknown shape.. so need to reshape it\n",
    "def reshape(image, mask):\n",
    "    image = tf.reshape(image, [512,512,3])\n",
    "    mask = tf.reshape(mask, [512,512])\n",
    "    return tf.cast(image, 'uint8'), tf.cast(mask, 'uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_mask_dims(image, mask):\n",
    "    mask = tf.expand_dims(mask, axis=2)\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shard_dataset(dataset=None, save_path=None):\n",
    "    count = 0\n",
    "    for x,y in dataset:\n",
    "        mini_shard = tf.data.Dataset.from_tensors((x,y))\n",
    "        mini_shard.save(f'{save_path}/repeat_{count}')\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_save = f'{root}/hold_processed'\n",
    "hold_shards_load = sorted(glob.glob(f'{root}/hold/*/*'))\n",
    "\n",
    "if not len(f'{hold_save}/*/*') < len(hold_shards_load):\n",
    "    for s in hold_shards_load:\n",
    "        folder = s.split('/')[-2]\n",
    "        ID = s.split('/')[-1]\n",
    "        shard_save = f'{hold_save}/{folder}/{ID}'\n",
    "        if not glob.glob(f'{shard_save}/*/*/*.snapshot'):\n",
    "            assert not glob.glob(f'{shard_save}/*/*.shard'), f'{shard_save} did not save properly'\n",
    "            shard = tf.data.Dataset.load(s)\n",
    "            shard = (\n",
    "                        shard.map(lambda i,m: expand_mask_dims(i,m), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                        .batch(1)\n",
    "                    )\n",
    "            shard.save(shard_save)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_save = f'{root}/external_processed'\n",
    "external_shards_load = sorted(glob.glob(f'{root}/external/*/*'))\n",
    "\n",
    "if not len(f'{external_save}/*/*') < len(external_shards_load):\n",
    "    for s in external_shards_load:\n",
    "        folder = s.split('/')[-2]\n",
    "        ID = s.split('/')[-1]\n",
    "        shard_save = f'{external_save}/{folder}/{ID}'\n",
    "        if not glob.glob(f'{shard_save}/*/*/*.snapshot'):\n",
    "            assert not glob.glob(f'{shard_save}/*/*.shard'), f'{shard_save} did not save properly'\n",
    "            shard = tf.data.Dataset.load(s)\n",
    "            shard = (\n",
    "                        shard.map(lambda i,m: expand_mask_dims(i,m), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                        .batch(1)\n",
    "                    )\n",
    "            shard.save(shard_save)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_save = f'{root}/train_Color.Basic.Aug_x{repeats}_.43.17'\n",
    "train_shards_load = sorted(glob.glob(f'{root}/train/*/*'))\n",
    "\n",
    "if len(glob.glob(f'{train_save}/*/*')) < len(train_shards_load):\n",
    "    for s in train_shards_load:\n",
    "        folder = s.split('/')[-2]\n",
    "        ID = s.split('/')[-1]\n",
    "        shard_save = f'{train_save}/{folder}/{ID}'\n",
    "        if not glob.glob(f'{shard_save}/repeat_{repeats-1}/*/*/*.snapshot'):\n",
    "            assert not glob.glob(f'{shard_save}/*/*.shard'), f'{shard_save} did not save properly'\n",
    "            test = tf.data.Dataset.load(s)\n",
    "            test = (\n",
    "                test.map(lambda i,m: tf.numpy_function(color_aug, [i,m], [tf.uint8, tf.uint8]), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                .map(lambda i,m: reshape(i,m), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            )\n",
    "            for d, m in test:\n",
    "                rgb_mean = tf.math.reduce_mean(d)\n",
    "                if (rgb_mean > 210) or (rgb_mean < 65) or (rgb_mean == None):\n",
    "                    shard = tf.data.Dataset.load(s)\n",
    "                    shard = (\n",
    "                        shard.repeat(repeats)\n",
    "                        .map(lambda i,m: aug(i,m), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                        .batch(1)\n",
    "                    )\n",
    "                    shard_dataset(shard, shard_save)\n",
    "                    restart_kernel()\n",
    "                    time.sleep(10)\n",
    "                else:\n",
    "                    shard = tf.data.Dataset.load(s)\n",
    "                    shard = (\n",
    "                        shard.repeat(repeats)\n",
    "                        .map(lambda i,m: tf.numpy_function(color_aug, [i,m], [tf.uint8, tf.uint8]), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                        .map(lambda i,m: reshape(i,m), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                        .map(lambda i,m: aug(i,m), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                        .batch(1)\n",
    "                    )\n",
    "                    shard_dataset(shard, shard_save)\n",
    "                    restart_kernel()\n",
    "                    time.sleep(10)\n",
    "        else:\n",
    "            continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
