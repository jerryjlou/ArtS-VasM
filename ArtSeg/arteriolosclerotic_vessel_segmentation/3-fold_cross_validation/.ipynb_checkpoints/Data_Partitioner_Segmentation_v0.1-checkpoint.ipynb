{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHANGELOG\n",
    "\n",
    "v0.1 11/13/2023\n",
    "- Divide into Training and Hold out only\n",
    "\n",
    "v0.0 7/14/2023\n",
    "- initiate from the wsi-arterio_batches_all_classification Data_Partitioner_v0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[ 2023-11-13 22:09:44 ] CUDA_VISIBLE_DEVICES automatically set to: 0           \n"
     ]
    }
   ],
   "source": [
    "from jarvis.utils.general import gpus\n",
    "gpus.autoselect()\n",
    "\n",
    "import glob, numpy as np, pandas as pd, tensorflow as tf, matplotlib.pyplot as plt, os, time\n",
    "\n",
    "# import after setting OPENCV_IO_MAX_IMAGE_PIXELS to 2^50\n",
    "os.environ[\"OPENCV_IO_MAX_IMAGE_PIXELS\"] = pow(2,50).__str__() \n",
    "import cv2\n",
    "\n",
    "import sys  \n",
    "sys.path.append('/home/jjlou/Jerry/jerry_packages')\n",
    "from jerry_utils import restart_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/jjlou/Jerry/wsi-arterio/arteriosclerotic_vessel_detection_and_fine_segmentation/Vessel_WallsLumen_Segmentation/data_test'\n",
    "shape = (512, 512) # resize all images to this shape\n",
    "shard_size = 20 #load this number of file addresses for each shard\n",
    "\n",
    "# Create hold out and valid lists of patient IDs\n",
    "hold_out_list = ['UCI-37-18', 'D-492', 'V019-B1_VP', 'V019_B3_VP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of all patches (dats) and list of all masks (msk)\n",
    "dats_pos_batches1and2 = sorted(glob.glob('/data/raw/wsi_arterio_2nd_alg/data/pos/*/*'))\n",
    "dats_pos_batch3 = sorted(glob.glob('/data/raw/wsi_arterio_2nd_alg/batch_3_data/annotations/pos/*/*'))\n",
    "dats_pos = dats_pos_batches1and2 + dats_pos_batch3\n",
    "\n",
    "msk_batches1and2 = sorted(glob.glob('/data/raw/wsi_arterio_2nd_alg/data/msk/*/*'))\n",
    "msk_batch3 = sorted(glob.glob('/data/raw/wsi_arterio_2nd_alg/batch_3_data/annotations/msk/*/*'))\n",
    "msk_pos = msk_batches1and2 + msk_batch3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dats=None, msk=None, shape=shape):\n",
    "    images = np.stack([tf.cast(cv2.resize(cv2.imread(d), shape), dtype='uint8') for d in dats])\n",
    "    \n",
    "    LMask = []\n",
    "    Mask = []\n",
    "    for m in msk:\n",
    "        msk_loaded = cv2.resize(cv2.imread(m), shape)\n",
    "        msk_loaded = msk_loaded[:,:,0]\n",
    "        msk_loaded = msk_loaded > 0\n",
    "        msk_loaded = msk_loaded.astype(float)\n",
    "        find_L = m.find('L')\n",
    "        if find_L != -1:\n",
    "            LMask.append(msk_loaded)\n",
    "        elif find_L == -1:\n",
    "            Mask.append(msk_loaded)\n",
    "\n",
    "    masks = []\n",
    "    for LMask, Mask in zip(LMask, Mask):\n",
    "        fm = Mask + LMask\n",
    "        fm = tf.cast(fm, dtype='uint8')\n",
    "        masks.append(fm)\n",
    "    masks = np.stack(masks)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, masks))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(dats=None, msk=None, path_root=None, path_modifier='', shape=shape, shard_size=shard_size):\n",
    "    iterate = [n for n in range(0, len(dats), shard_size)]\n",
    "    for i in iterate:\n",
    "        save_path = f'{path_root}/{path_modifier}shard_{iterate.index(i)}'\n",
    "        if glob.glob(f'{save_path}/*/*/*.snapshot'):\n",
    "            continue\n",
    "        else:\n",
    "            shard_dats = dats[i:i+shard_size]\n",
    "            shard_msk = msk[i*2:(i+shard_size)*2]\n",
    "            shard = load_data(dats=shard_dats, msk=shard_msk)\n",
    "\n",
    "            # Only need a path_modifier if \"concatenating\" datasets by saving to the same path_root\n",
    "            shard.save(save_path) \n",
    "            if i+shard_size-1 < len(dats):\n",
    "                record = {'start': dats[i], 'stop': dats[i+shard_size-1]}\n",
    "            else:\n",
    "                record = {'start': dats[i], 'stop': dats[len(dats)-1]}\n",
    "            record = pd.DataFrame.from_dict(record, orient='index')\n",
    "            record.to_csv(f'{save_path}/record.csv') \n",
    "            \n",
    "            restart_kernel()\n",
    "            time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_out_dats_pos = []\n",
    "hold_out_msk_pos = []\n",
    "\n",
    "# Find all instances of the hold out patient IDs in dats and msk then save in corresponding list\n",
    "for h in hold_out_list:\n",
    "    for d in dats_pos:\n",
    "        if d.find(h) != -1:\n",
    "            hold_out_dats_pos.append(d)\n",
    "        else:\n",
    "            continue\n",
    "    for m in msk_pos:\n",
    "        if m.find(h) != -1:\n",
    "            hold_out_msk_pos.append(m)\n",
    "        else:\n",
    "            continue   \n",
    "            \n",
    "hold_path = f'{root}/hold_out'\n",
    "hold_num = len([n for n in range(0, len(hold_out_dats_pos), shard_size)])\n",
    "if not glob.glob(f'{hold_path}/shard_{hold_num-1}/*/*/*.snapshot'):\n",
    "    process_data(\n",
    "        dats=hold_out_dats_pos, \n",
    "        msk=hold_out_msk_pos,   \n",
    "        path_root=hold_path, \n",
    "        path_modifier='')\n",
    "\n",
    "# Remove hold out dats and msks from original list\n",
    "for d in hold_out_dats_pos:\n",
    "    dats_pos.remove(d)\n",
    "for m in hold_out_msk_pos:\n",
    "    msk_pos.remove(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = f'{root}/train'\n",
    "train_num = len([n for n in range(0, len(dats_pos), shard_size)])\n",
    "if not glob.glob(f'{train_path}/shard_{train_num-1}/*/*/*.snapshot'):\n",
    "    process_data(\n",
    "        dats=dats_pos, \n",
    "        msk=msk_pos,   \n",
    "        path_root=train_path, \n",
    "        path_modifier='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
