{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHANGELOG\n",
    "\n",
    "v0.3 10/25/2023\n",
    "- use jerry_losses v0.1 focal_dice_like_loss_multiclass_weighted\n",
    "\n",
    "v0.2 10/25/2023\n",
    "- use dice_multiclass because dice_like seems to be not working for the lumen\n",
    "\n",
    "v0.1 10/23/2023\n",
    "- use jerry_losses v0.0 dice_like\n",
    "\n",
    "v0.1 10/17/2023\n",
    "- add in all metrics that I want to eval\n",
    "- then try using model.evaluate later instead of the custom one\n",
    "- dice_like doesn't seem to be working\n",
    "\n",
    "v0.0 7/13/2023\n",
    "- Initialize from copy of Unfrozen_EfficientNetV2L_deeper2_v0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[ 2023-10-26 20:06:41 ] WARNING 1 GPU device(s) requested but only 0 available \n"
     ]
    }
   ],
   "source": [
    "from jarvis.utils.general import gpus\n",
    "gpus.autoselect()\n",
    "\n",
    "import numpy as np, pandas as pd, tensorflow as tf, os, keras.backend as K, time\n",
    "from tensorflow.keras import Input, Model, layers, optimizers, losses, callbacks, utils\n",
    "from jarvis.train import custom\n",
    "from IPython.display import clear_output, HTML, Javascript, display\n",
    "\n",
    "import sys  \n",
    "sys.path.append('/home/jjlou/Jerry/jerry_packages')\n",
    "from jerry_utils import restart_kernel, load_dataset, save_dataset\n",
    "import jerry_losses, jerry_metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Attention_Unfrozen_EfficientNetV2L_v0.3'\n",
    "batch_size = 30\n",
    "epochs = 500\n",
    "learning_rate = 1e-2/5\n",
    "learning_ratio = 0.99\n",
    "loss = jerry_losses.focal_dice_like_loss_multiclass_weighted\n",
    "metrics = [\n",
    "    jerry_metrics.dice_metric(cls=1),\n",
    "    jerry_metrics.hausdorff_metric(cls=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/jjlou/Jerry/wsi-arterio/arteriosclerotic_vessel_detection_and_fine_segmentation/Vessel_WallsLumen_Segmentation/data'\n",
    "\n",
    "Valid0 = ['D-436','D-870', 'D-343']\n",
    "Valid1 = ['D-297', 'D-916', 'UCI-15-12']\n",
    "Valid2 = ['D-322', 'D-562']\n",
    "\n",
    "Valid = [Valid0, Valid1, Valid2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Attention_Unfrozen_EfficientNetV2L ####\n",
    "\n",
    "def prepare_model(shape=(512, 512, 3)):\n",
    "    \n",
    "    # --- Transfer learning\n",
    "    base_model = tf.keras.applications.EfficientNetV2L(input_shape=shape, include_top=False)\n",
    "    base_model.trainable = True\n",
    "\n",
    "    # Use the activations of these layers\n",
    "    layer_names = [\n",
    "        'block1d_add',               # 256x256\n",
    "        'block2g_add',               # 128x128, layer 120\n",
    "        'block4a_expand_activation', # 64x64\n",
    "        'block6a_expand_activation', # 32x32, layer 551\n",
    "        'top_activation'             # 16x16\n",
    "    ]\n",
    "    \n",
    "    #Total 1028 layers, layer 551 corresponds to layer_names[3] so freeze the first 4 layers\n",
    "    fine_tune_at = 551 \n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
    "    down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n",
    "    \n",
    "    # --- Dr. Chang's base setup\n",
    "    kwargs = {    \n",
    "        'kernel_size':(3,3), \n",
    "        'padding':'same',\n",
    "        'kernel_initializer':'he_normal'}\n",
    "    \n",
    "    # --- Define functions\n",
    "    conv = lambda x, filters, strides=1, k=(3,3), r=1 : layers.SeparableConv2D(\n",
    "        filters=filters,\n",
    "        strides=strides,\n",
    "        dilation_rate=r,\n",
    "        kernel_size=k,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "    \n",
    "    tran = lambda x, filters : layers.Conv2DTranspose(\n",
    "        filters=filters,\n",
    "        strides=(2, 2),\n",
    "        **kwargs)(x)\n",
    "\n",
    "    norm = lambda x : layers.BatchNormalization()(x)\n",
    "    relu = lambda x : layers.LeakyReLU()(x)\n",
    "    \n",
    "    conv1 = lambda filters, x : relu(norm(conv(x, filters, strides=1)))\n",
    "    conv2 = lambda filters, x : relu(norm(conv(x, filters, strides=(2, 2))))\n",
    "    \n",
    "    tran2 = lambda filters, x : relu(norm(tran(x, filters)))\n",
    "    \n",
    "    concat = lambda x, y : layers.Concatenate()([x,y])\n",
    "    \n",
    "    # --- Attention\n",
    "    def create_attention_gating(g, x, subsamp=(2, 2), **kwargs):\n",
    "        # --- Prepare g (gating signal)\n",
    "        g = layers.SeparableConv2D(filters=x.shape[-1], kernel_size=1, strides=1)(g)\n",
    "        # --- Prepare x (skip connection signal)\n",
    "        x = layers.SeparableConv2D(filters=x.shape[-1], kernel_size=1, strides=subsamp)(x)\n",
    "        # --- Add and ReLU\n",
    "        a = layers.LeakyReLU()(g + x)\n",
    "        # --- Compress to single feature map + sigmoid\n",
    "        a = layers.SeparableConv2D(filters=1, strides=1, kernel_size=1, activation='sigmoid')(a)\n",
    "        # --- Resampler\n",
    "        if subsamp != (1, 1):\n",
    "            a = layers.UpSampling2D(size=subsamp)(a)\n",
    "        return a\n",
    "    \n",
    "    # --- Create model\n",
    "    inputs = tf.keras.layers.Input(shape=shape, dtype='float32')\n",
    "    down = down_stack(inputs) # transfer layers from pre-trained model\n",
    "    \n",
    "    # Encoder\n",
    "    l0 = conv1(8, inputs)\n",
    "    l1 = conv1(16,concat(down[0],conv2(16, l0)))\n",
    "    l2 = conv1(32,concat(down[1],conv2(32, l1)))\n",
    "    l3 = conv1(48,concat(down[2],conv2(48, l2)))\n",
    "    l4 = conv1(96,concat(down[3],conv2(96, l3)))\n",
    "    l5 = conv1(64,concat(down[4],conv2(64, l4)))\n",
    "    l6 = conv1(72,conv2(72,l5))\n",
    "    l7 = conv1(96,conv2(96,l6))\n",
    "    \n",
    "    # Decoder  \n",
    "    l = [l6, l5, l4, l3, l2, l1, l0]\n",
    "    f = [96, 72, 64, 48, 32, 16, 8]\n",
    "    g = l7\n",
    "    for i in range(len(l)):\n",
    "        x = l[i]\n",
    "        a = create_attention_gating(g, x)\n",
    "        g = conv1(f[i], concat(tran2(f[i], g), x * a))\n",
    "\n",
    "    # --- Create logits\n",
    "    logits = conv1(3, g)\n",
    "\n",
    "    # --- Create model\n",
    "    model = Model(inputs=inputs, outputs=logits) \n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(dataset=None, num=1):\n",
    "    for image, mask in dataset.take(num):\n",
    "        pred = model.predict(image)\n",
    "        pred = tf.math.argmax(pred, axis=-1)\n",
    "        pred = pred[..., tf.newaxis]\n",
    "        mask = tf.stack([mask, mask, mask], -1) \n",
    "        mask = tf.squeeze(mask, 3) # allow np stacking of all arrays later on by converting to same shape\n",
    "        pred = tf.stack([pred, pred, pred], -1)\n",
    "        pred = np.squeeze(pred, 3)\n",
    "        progression.append([image, mask, pred])\n",
    "\n",
    "#Save observations of how the model trains after each epoch\n",
    "class SaveCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        clear_output(wait=True)\n",
    "        make_predictions(train,1) # Sets what to look at when examining pics of training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in Valid:\n",
    "    v_path = f\"{root}/K{Valid.index(v)}\"\n",
    "    train_path = f\"{v_path}/Train_K{Valid.index(v)}_Color.BasicMorph.Aug\"  \n",
    "    valid_path = f\"{v_path}/Valid_K{Valid.index(v)}_Color.BasicMorph.Aug\" \n",
    "    model_path = f'{v_path}/models/{name}_Color.BasicMorph.Aug_K{Valid.index(v)}.hdf5'\n",
    "    history_path = f'{v_path}/models/{name}_Color.BasicMorph.Aug_K{Valid.index(v)}.csv'\n",
    "    progression_path = f'{v_path}/models/{name}_Color.BasicMorph.Aug_K{Valid.index(v)}.npy'\n",
    "    \n",
    "    if not os.path.exists(f'{v_path}/models'):\n",
    "        os.mkdir(f'{v_path}/models')\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        continue    \n",
    "    else:\n",
    "        train = load_dataset(train_path) \n",
    "        valid = load_dataset(valid_path)\n",
    "        \n",
    "        steps_per_epoch = int(len(train)//batch_size)\n",
    "        validation_steps = int(len(valid)//batch_size)\n",
    "        \n",
    "        model = prepare_model()\n",
    "        progression = []\n",
    "        \n",
    "        # --- Learning rate scheduler\n",
    "        lr_scheduler = callbacks.LearningRateScheduler(lambda epoch, lr : lr * learning_ratio)\n",
    "        \n",
    "        # --- Compile model\n",
    "        model.compile(\n",
    "            optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss=loss,\n",
    "            metrics=metrics)\n",
    "        \n",
    "        # --- Train\n",
    "        model_history = model.fit(\n",
    "            x=train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_data=valid,\n",
    "            callbacks=[lr_scheduler, SaveCallback()])\n",
    "        \n",
    "        history = pd.DataFrame.from_dict(model_history.history)\n",
    "        history.to_csv(history_path)\n",
    "        \n",
    "        progression = np.stack(progression)\n",
    "        np.save(progression_path, progression)\n",
    "    \n",
    "        model.save(model_path)\n",
    "        \n",
    "        restart_kernel()\n",
    "        time.sleep(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
