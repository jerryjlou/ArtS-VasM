{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHANGELOG\n",
    "\n",
    "v0.0 6/16/23:\n",
    "- BasicAug\n",
    "- batches_all\n",
    "\n",
    "v0.1 6/19/23\n",
    "- added in pearson correlation\n",
    "\n",
    "v0.2 6/19/23:\n",
    "- added hausdorff distance\n",
    "- added IoU\n",
    "- added AUC ROC\n",
    "- added AUC PR\n",
    "\n",
    "v0.3 6/24/23\n",
    "- added access to jerry_losses.py\n",
    "\n",
    "v0.4 7/3/2023\n",
    "- added in restart run all\n",
    "\n",
    "v0.5 7/4/2023\n",
    "- added in if and elif clauses to check for prior existing files before running things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[ 2024-01-08 22:24:52 ] CUDA_VISIBLE_DEVICES automatically set to: 2           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjlou/.local/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.6.0 and strictly below 2.9.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from jarvis.utils.general import gpus\n",
    "gpus.autoselect()\n",
    "\n",
    "import glob, numpy as np, pandas as pd, tensorflow as tf, os, scipy\n",
    "from tensorflow.keras import Input, Model, layers\n",
    "from pathlib import Path\n",
    "from jarvis.train import models, custom\n",
    "from jarvis.utils.display import imshow\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from IPython.display import clear_output, HTML, Javascript, display\n",
    "\n",
    "import sys  \n",
    "sys.path.append('/home/jjlou/Jerry/jerry_packages')\n",
    "from jerry_utils import restart_kernel, load_dataset, load_dataset_v1\n",
    "import jerry_losses, jerry_metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Attention_Unfrozen_EfficientNetV2L_data_v0.0.py'\n",
    "root = '/home/jjlou/Jerry/wsi-arterio/arteriosclerotic_vessel_detection_and_fine_segmentation/Vessel_WallsLumen_Segmentation/data'\n",
    "\n",
    "custom_objects = {\n",
    "    'dice_all': jerry_metrics.dice_metric(cls=1),\n",
    "    'hausdorff_all': jerry_metrics.hausdorff_metric(cls=1),\n",
    "    'focal_dice_like_loss_multiclass_weighted': jerry_losses.focal_dice_like_loss_multiclass_weighted\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Valid0 = ['D-436','D-870', 'D-343']\n",
    "Valid1 = ['D-297', 'D-916', 'UCI-15-12']\n",
    "Valid2 = ['D-322', 'D-562']\n",
    "\n",
    "Valid = [Valid0, Valid1, Valid2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_pred(true, pred, c):\n",
    "    true = true.numpy()\n",
    "    if c == None:\n",
    "        true = true[..., 0]\n",
    "        pred = np.argmax(pred, axis=-1)\n",
    "    else:\n",
    "        true = true[..., 0] == c\n",
    "        pred = np.argmax(pred, axis=-1) == c\n",
    "    return true, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_roc = tf.keras.metrics.AUC(curve='ROC')\n",
    "calc_pr = tf.keras.metrics.AUC(curve='PR')\n",
    "def AUC(y_true, y_pred, c=None):\n",
    "    true, pred = true_pred(y_true, y_pred, c)\n",
    "    calc_roc.update_state(true, pred)\n",
    "    calc_pr.update_state(true, pred)\n",
    "    roc = calc_roc.result().numpy()\n",
    "    pr = calc_pr.result().numpy()\n",
    "    calc_roc.reset_state()\n",
    "    calc_pr.reset_state()\n",
    "    return float(roc), float(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.keras.metrics.MeanIoU(num_classes=3)\n",
    "def mIoU(y_true, y_pred, c=None):\n",
    "    true, pred = true_pred(y_true, y_pred, c)\n",
    "    m.update_state(true, pred)\n",
    "    r = m.result().numpy()\n",
    "    m.reset_state()\n",
    "    return float(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hausdorff(y_true, y_pred, c=None):\n",
    "    true, pred = true_pred(y_true, y_pred, c)\n",
    "    true = np.squeeze(true)\n",
    "    pred = np.squeeze(pred)\n",
    "    d = directed_hausdorff(true, pred)[0]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice(y_true, y_pred, c=None):\n",
    "    true, pred = true_pred(y_true, y_pred, c)\n",
    "    true = true.astype('int64')\n",
    "    A = np.count_nonzero(true & pred) * 2\n",
    "    B = np.count_nonzero(true) + np.count_nonzero(pred)\n",
    "    if np.count_nonzero(true) == 0 and np.count_nonzero(pred) == 0:\n",
    "        A = 1\n",
    "        B = 1\n",
    "    return A / B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson(y_true, y_pred, c=None):\n",
    "    true, pred = true_pred(y_true, y_pred, c)\n",
    "    true = true.flatten()\n",
    "    pred = pred.flatten()\n",
    "    pear, p = scipy.stats.pearsonr(pred,true)\n",
    "    return pear, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vessel_lumen():\n",
    "    # Vessel and lumen\n",
    "    dsc_train_vl = []\n",
    "    train_vl_pearson = []\n",
    "    train_vl_p = []\n",
    "    train_vl_hausdorff = []\n",
    "    train_vl_mIoU = []\n",
    "    train_vl_ROC = []\n",
    "    train_vl_PR = []\n",
    "    for x,y in train:\n",
    "        logits = model.predict(x) \n",
    "        if type(logits) is dict:\n",
    "            logits = logits['r1']\n",
    "        dsc_train_vl.append(dice(y, logits)) # dice\n",
    "        train_vl_hausdorff.append(hausdorff(y, logits)) # hausdorff\n",
    "        train_vl_mIoU.append(mIoU(y, logits)) #mIoU\n",
    "        roc, pr = AUC(y, logits) #AUC ROC and PR\n",
    "        train_vl_ROC.append(roc)\n",
    "        train_vl_PR.append(pr)\n",
    "        pear, p = pearson(y, logits) # pearson\n",
    "        train_vl_pearson.append(pear)\n",
    "        train_vl_p.append(p) \n",
    "\n",
    "    dsc_valid_vl = []\n",
    "    valid_vl_pearson = []\n",
    "    valid_vl_p = []\n",
    "    valid_vl_hausdorff = []\n",
    "    valid_vl_mIoU = []\n",
    "    valid_vl_ROC = []\n",
    "    valid_vl_PR = []\n",
    "    for x,y in valid:\n",
    "        logits = model.predict(x) \n",
    "        if type(logits) is dict:\n",
    "            logits = logits['r1']\n",
    "        dsc_valid_vl.append(dice(y, logits))\n",
    "        valid_vl_hausdorff.append(hausdorff(y, logits))\n",
    "        valid_vl_mIoU.append(mIoU(y, logits))\n",
    "        roc, pr = AUC(y, logits)\n",
    "        valid_vl_ROC.append(roc)\n",
    "        valid_vl_PR.append(pr)\n",
    "        pear, p = pearson(y, logits)\n",
    "        valid_vl_pearson.append(pear)\n",
    "        valid_vl_p.append(p)            \n",
    "\n",
    "    df_train_vl = pd.DataFrame(index=np.arange(len(dsc_train_vl)))\n",
    "    df_valid_vl = pd.DataFrame(index=np.arange(len(dsc_valid_vl)))\n",
    "    df_train_vl['Train dice score'] = dsc_train_vl\n",
    "    df_valid_vl['Valid dice score'] = dsc_valid_vl\n",
    "    df_train_vl['Train pearson'] = train_vl_pearson\n",
    "    df_train_vl['Train p'] = train_vl_p\n",
    "    df_valid_vl['Valid pearson'] = valid_vl_pearson\n",
    "    df_valid_vl['Valid p'] = valid_vl_p\n",
    "    df_train_vl['Train hausdorff'] = train_vl_hausdorff\n",
    "    df_valid_vl['Valid hausdorff'] = valid_vl_hausdorff\n",
    "    df_train_vl['Train mIoU'] = train_vl_mIoU\n",
    "    df_valid_vl['Valid mIoU'] = valid_vl_mIoU\n",
    "    df_train_vl['Train ROC'] = train_vl_ROC\n",
    "    df_train_vl['Train PR'] = train_vl_PR\n",
    "    df_valid_vl['Valid ROC'] = valid_vl_ROC\n",
    "    df_valid_vl['Valid PR'] = valid_vl_PR\n",
    "    df_train_vl.to_csv(f'{save_path}_train_vl.csv')\n",
    "    df_valid_vl.to_csv(f'{save_path}_valid_vl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vessel():\n",
    "   # Vessel\n",
    "    dsc_train_v = []\n",
    "    train_v_pearson = []\n",
    "    train_v_p = []\n",
    "    train_v_hausdorff = []\n",
    "    train_v_mIoU = []\n",
    "    train_v_ROC = []\n",
    "    train_v_PR = []\n",
    "    for x,y in train:\n",
    "        logits = model.predict(x) \n",
    "        if type(logits) is dict:\n",
    "            logits = logits['r1']\n",
    "        dsc_train_v.append(dice(y, logits,c=1))\n",
    "        train_v_hausdorff.append(hausdorff(y, logits, c=1))\n",
    "        train_v_mIoU.append(mIoU(y, logits, c=1))\n",
    "        roc, pr = AUC(y, logits, c=1) \n",
    "        train_v_ROC.append(roc)\n",
    "        train_v_PR.append(pr)\n",
    "        pear, p = pearson(y, logits, c=1)\n",
    "        train_v_pearson.append(pear)\n",
    "        train_v_p.append(p) \n",
    "\n",
    "    dsc_valid_v = []\n",
    "    valid_v_pearson = []\n",
    "    valid_v_p = []\n",
    "    valid_v_hausdorff = []\n",
    "    valid_v_mIoU = []\n",
    "    valid_v_ROC = []\n",
    "    valid_v_PR = []\n",
    "    for x,y in valid:\n",
    "        logits = model.predict(x) \n",
    "        if type(logits) is dict:\n",
    "            logits = logits['r1']\n",
    "        dsc_valid_v.append(dice(y, logits,c=1))\n",
    "        valid_v_hausdorff.append(hausdorff(y, logits, c=1))\n",
    "        valid_v_mIoU.append(mIoU(y, logits, c=1))\n",
    "        roc, pr = AUC(y, logits, c=1) \n",
    "        valid_v_ROC.append(roc)\n",
    "        valid_v_PR.append(pr)\n",
    "        pear, p = pearson(y, logits, c=1)\n",
    "        valid_v_pearson.append(pear)\n",
    "        valid_v_p.append(p) \n",
    "\n",
    "    df_train_v = pd.DataFrame(index=np.arange(len(dsc_train_v)))\n",
    "    df_valid_v = pd.DataFrame(index=np.arange(len(dsc_valid_v)))\n",
    "    df_train_v['Train dice score'] = dsc_train_v\n",
    "    df_valid_v['Valid dice score'] = dsc_valid_v\n",
    "    df_train_v['Train pearson'] = train_v_pearson\n",
    "    df_train_v['Train p'] = train_v_p\n",
    "    df_valid_v['Valid pearson'] = valid_v_pearson\n",
    "    df_valid_v['Valid p'] = valid_v_p\n",
    "    df_train_v['Train hausdorff'] = train_v_hausdorff\n",
    "    df_valid_v['Valid hausdorff'] = valid_v_hausdorff\n",
    "    df_train_v['Train mIoU'] = train_v_mIoU\n",
    "    df_valid_v['Valid mIoU'] = valid_v_mIoU\n",
    "    df_train_v['Train ROC'] = train_v_ROC\n",
    "    df_train_v['Train PR'] = train_v_PR\n",
    "    df_valid_v['Valid ROC'] = valid_v_ROC\n",
    "    df_valid_v['Valid PR'] = valid_v_PR\n",
    "    df_train_v.to_csv(f'{save_path}_train_v.csv')\n",
    "    df_valid_v.to_csv(f'{save_path}_valid_v.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lumen():\n",
    "    # Lumen\n",
    "    dsc_train_l = []\n",
    "    train_l_pearson = []\n",
    "    train_l_p = []\n",
    "    train_l_hausdorff = []\n",
    "    train_l_mIoU = []\n",
    "    train_l_ROC = []\n",
    "    train_l_PR = []\n",
    "    for x,y in train:\n",
    "        logits = model.predict(x) \n",
    "        if type(logits) is dict:\n",
    "            logits = logits['r1']\n",
    "        dsc_train_l.append(dice(y, logits,c=2))\n",
    "        train_l_hausdorff.append(hausdorff(y, logits, c=2))\n",
    "        train_l_mIoU.append(mIoU(y, logits, c=2))\n",
    "        roc, pr = AUC(y, logits, c=2) \n",
    "        train_l_ROC.append(roc)\n",
    "        train_l_PR.append(pr)\n",
    "        pear, p = pearson(y, logits, c=2)\n",
    "        train_l_pearson.append(pear)\n",
    "        train_l_p.append(p) \n",
    "\n",
    "    dsc_valid_l = []\n",
    "    valid_l_pearson = []\n",
    "    valid_l_p = []\n",
    "    valid_l_hausdorff = []\n",
    "    valid_l_mIoU = []\n",
    "    valid_l_ROC = []\n",
    "    valid_l_PR = []\n",
    "    for x,y in valid:\n",
    "        logits = model.predict(x) \n",
    "        if type(logits) is dict:\n",
    "            logits = logits['r1']\n",
    "        dsc_valid_l.append(dice(y, logits,c=2))\n",
    "        valid_l_hausdorff.append(hausdorff(y, logits, c=2))\n",
    "        valid_l_mIoU.append(mIoU(y, logits, c=2))\n",
    "        roc, pr = AUC(y, logits, c=2) \n",
    "        valid_l_ROC.append(roc)\n",
    "        valid_l_PR.append(pr)\n",
    "        pear, p = pearson(y, logits, c=2)\n",
    "        valid_l_pearson.append(pear)\n",
    "        valid_l_p.append(p) \n",
    "\n",
    "    df_train_l = pd.DataFrame(index=np.arange(len(dsc_train_l)))\n",
    "    df_valid_l = pd.DataFrame(index=np.arange(len(dsc_valid_l)))\n",
    "    df_train_l['Train dice score'] = dsc_train_l\n",
    "    df_valid_l['Valid dice score'] = dsc_valid_l\n",
    "    df_train_l['Train pearson'] = train_l_pearson\n",
    "    df_train_l['Train p'] = train_l_p\n",
    "    df_valid_l['Valid pearson'] = valid_l_pearson\n",
    "    df_valid_l['Valid p'] = valid_l_p\n",
    "    df_train_l['Train hausdorff'] = train_l_hausdorff\n",
    "    df_valid_l['Valid hausdorff'] = valid_l_hausdorff\n",
    "    df_train_l['Train mIoU'] = train_l_mIoU\n",
    "    df_valid_l['Valid mIoU'] = valid_l_mIoU\n",
    "    df_train_l['Train ROC'] = train_l_ROC\n",
    "    df_train_l['Train PR'] = train_l_PR\n",
    "    df_valid_l['Valid ROC'] = valid_l_ROC\n",
    "    df_valid_l['Valid PR'] = valid_l_PR\n",
    "    df_train_l.to_csv(f'{save_path}_train_l.csv')\n",
    "    df_valid_l.to_csv(f'{save_path}_valid_l.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at /home/jjlou/Jerry/wsi-arterio/arteriosclerotic_vessel_detection_and_fine_segmentation/Vessel_WallsLumen_Segmentation/data/K0/models/Attention_Unfrozen_EfficientNetV2L_data_v0.0.hdf5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-11ea7f8d5ef5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# load model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{save_path}_valid_vl.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                         raise IOError(\n\u001b[0m\u001b[1;32m    227\u001b[0m                             \u001b[0;34mf\"No file or directory found at {filepath_str}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                         )\n",
      "\u001b[0;31mOSError\u001b[0m: No file or directory found at /home/jjlou/Jerry/wsi-arterio/arteriosclerotic_vessel_detection_and_fine_segmentation/Vessel_WallsLumen_Segmentation/data/K0/models/Attention_Unfrozen_EfficientNetV2L_data_v0.0.hdf5"
     ]
    }
   ],
   "source": [
    "for v in Valid:\n",
    "    v_path = f\"{root}/K{Valid.index(v)}\"\n",
    "    train1 = glob.glob(f\"{v_path}/Train_K{Valid.index(v)}_Color.Basic.Aug_x50_.43.17/*/*\")\n",
    "    train2 = glob.glob(f'{v_path}/Train_K{Valid.index(v)}_Gray.Basic.Aug_x10/*/*')\n",
    "    train_path = train1 + train2\n",
    "    valid_path = glob.glob(f\"{v_path}/Valid_K{Valid.index(v)}_Color.BasicMorph.Aug/*\") \n",
    "    model_path = f'{v_path}/models/{name}.hdf5'\n",
    "    save_path = f'{v_path}/models/{name}_K{Valid.index(v)}'\n",
    "    \n",
    "    if os.path.exists(f'{save_path}_valid_l.csv'):\n",
    "        continue\n",
    "    else:\n",
    "        # load data\n",
    "        train = load_dataset_v1(train_path) \n",
    "        valid = load_dataset(valid_path)\n",
    "        \n",
    "        # load model\n",
    "        model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)\n",
    "    \n",
    "        if not os.path.exists(f'{save_path}_valid_vl.csv'):\n",
    "            vessel_lumen()\n",
    "            vessel()\n",
    "            lumen()\n",
    "            restart_kernel()\n",
    "            \n",
    "        elif not os.path.exists(f'{save_path}_valid_v.csv'):\n",
    "            vessel()\n",
    "            lumen()\n",
    "            restart_kernel()\n",
    "        \n",
    "        else:\n",
    "            lumen()\n",
    "            restart_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
