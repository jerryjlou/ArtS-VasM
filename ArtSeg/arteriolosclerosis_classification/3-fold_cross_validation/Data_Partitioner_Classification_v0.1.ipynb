{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHANGELOG\n",
    "\n",
    "v0.1 10/13/2023\n",
    "- change to classification version\n",
    "\n",
    "v0.0 7/14/2023\n",
    "- initiate from the wsi-arterio_batches_all_classification Data_Partitioner_v0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[ 2023-10-17 18:32:44 ] WARNING 1 GPU device(s) requested but only 0 available \n"
     ]
    }
   ],
   "source": [
    "from jarvis.utils.general import gpus\n",
    "gpus.autoselect()\n",
    "\n",
    "import glob, numpy as np, pandas as pd, tensorflow as tf, matplotlib.pyplot as plt, os, gc\n",
    "from jarvis.utils.display import imshow\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import after setting OPENCV_IO_MAX_IMAGE_PIXELS to 2^50\n",
    "os.environ[\"OPENCV_IO_MAX_IMAGE_PIXELS\"] = pow(2,50).__str__() \n",
    "import cv2\n",
    "\n",
    "import sys  \n",
    "sys.path.append('/home/jjlou/Jerry/jerry_packages')\n",
    "from jerry_utils import restart_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/jjlou/Jerry/wsi-arterio/arteriosclerotic_vessel_detection_and_fine_segmentation/Arteriolosclerosis_classification/data'\n",
    "shape = (512, 512) # resize all images to this shape\n",
    "shard_size = 50\n",
    "\n",
    "# Create hold out and valid lists of patient IDs\n",
    "hold_out_list = ['UCI-37-18', 'D-492', 'V019-B1_VP', 'V019_B3_VP']\n",
    "\n",
    "Valid0 = ['D-436','D-870', 'D-343']\n",
    "Valid1 = ['D-297', 'D-916', 'UCI-15-12']\n",
    "Valid2 = ['D-322', 'D-562']\n",
    "\n",
    "Valid = [Valid0, Valid1, Valid2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of all patches (dats) and list of all masks (msk)\n",
    "dats_pos_batches1and2 = sorted(glob.glob('/data/raw/wsi_arterio_2nd_alg/data/pos/*/*'))\n",
    "dats_pos_batch3 = sorted(glob.glob('/data/raw/wsi_arterio_2nd_alg/batch_3_data/annotations/pos/*/*'))\n",
    "dats_pos = dats_pos_batches1and2 + dats_pos_batch3\n",
    "\n",
    "dats_neg_batches1and2 = sorted(glob.glob('/data/raw/wsi_arterio_2nd_alg/data/neg/*/*'))\n",
    "dats_neg_batch3 = sorted(glob.glob('/data/raw/wsi_arterio_2nd_alg/batch_3_data/annotations/neg/*/*'))\n",
    "dats_neg = dats_neg_batches1and2 + dats_neg_batch3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dats=None, cls=None, shape=None):\n",
    "    images = [tf.cast(cv2.resize(cv2.imread(d), shape), dtype='uint8') for d in dats]\n",
    "    labels = [tf.cast(cls, dtype='uint8') for _ in range(len(images))]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(dats=None, cls=None, shape=None, shard_size=shard_size, path_root=None, path_modifier=''):\n",
    "    iterate = [n for n in range(0, len(dats), shard_size)]\n",
    "    for i in iterate:\n",
    "        save_path = f'{path_root}/{path_modifier}shard_{iterate.index(i)}'\n",
    "        if not glob.glob(f'{save_path}/*/*/*.snapshot'):\n",
    "            assert not glob.glob(f'{save_path}/*/*.shard'), f'{save_path} did not save properly'\n",
    "            \n",
    "            shard_dats = dats[i:i+shard_size]\n",
    "            shard = load_data(dats=shard_dats, cls=cls, shape=shape)\n",
    "\n",
    "            # Only need a path_modifier if \"concatenating\" datasets by saving to the same path_root\n",
    "            save_path = f'{path_root}/{path_modifier}shard_{iterate.index(i)}'\n",
    "            shard.save(save_path) \n",
    "            if i+shard_size-1 < len(dats):\n",
    "                record = {'start': dats[i], 'stop': dats[i+shard_size-1]}\n",
    "            else:\n",
    "                record = {'start': dats[i], 'stop': dats[len(dats)-1]}\n",
    "            record = pd.DataFrame.from_dict(record, orient='index')\n",
    "            record.to_csv(f'{save_path}/record.csv') \n",
    "            \n",
    "            restart_kernel()\n",
    "            time.sleep(10)\n",
    "\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_out_dats_pos = []\n",
    "hold_out_dats_neg = []\n",
    "\n",
    "# Find all instances of the hold out patient IDs in dats and msk then save in corresponding list\n",
    "for h in hold_out_list:\n",
    "    for d in dats_pos:\n",
    "        if d.find(h) != -1:\n",
    "            hold_out_dats_pos.append(d)\n",
    "        else:\n",
    "            continue \n",
    "    for dn in dats_neg:\n",
    "        if dn.find(h) != -1:\n",
    "            hold_out_dats_neg.append(dn)\n",
    "            \n",
    "hold_path = f'{root}/hold_out'   \n",
    "hold_num_pos = len([n for n in range(0, len(hold_out_dats_pos), shard_size)]) - 1\n",
    "hold_num_neg = len([n for n in range(0, len(hold_out_dats_neg), shard_size)]) - 1\n",
    "\n",
    "if not glob.glob(f'{hold_path}/pos_shard_{hold_num_pos}/*/*/*.snapshot'):\n",
    "    \n",
    "    # process positive hold out\n",
    "    process_data(\n",
    "        dats=hold_out_dats_pos, \n",
    "        cls=1, \n",
    "        shape=shape, \n",
    "        shard_size=shard_size, \n",
    "        path_root=hold_path, \n",
    "        path_modifier='pos_')\n",
    "\n",
    "if not glob.glob(f'{hold_path}/neg_shard_{hold_num_neg}/*/*/*.snapshot'):\n",
    "    \n",
    "    # process negative hold out\n",
    "    process_data(\n",
    "        dats=hold_out_dats_neg, \n",
    "        cls=0, \n",
    "        shape=shape, \n",
    "        shard_size=shard_size, \n",
    "        path_root=hold_path, \n",
    "        path_modifier='neg_')\n",
    "\n",
    "# Remove hold out dats and msks from original list\n",
    "for d in hold_out_dats_pos:\n",
    "    dats_pos.remove(d)\n",
    "for n in hold_out_dats_neg:\n",
    "    dats_neg.remove(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in Valid:\n",
    "    v_path = f\"{root}/K{Valid.index(v)}\"\n",
    "    train_path = f\"{v_path}/Train_K{Valid.index(v)}_raw\"\n",
    "    valid_path = f\"{v_path}/Valid_K{Valid.index(v)}_raw\"\n",
    "     \n",
    "    #Valid data\n",
    "    valid_dats_pos = []\n",
    "    valid_dats_neg = []   \n",
    "    for i in v: #for each patient ID in v, which contains all patient IDs of validation, find files and append to list\n",
    "        for p in dats_pos:\n",
    "            if p.find(i) != -1:\n",
    "                valid_dats_pos.append(p)\n",
    "            else:\n",
    "                continue\n",
    "        for n in dats_neg:\n",
    "            if n.find(i) != -1:\n",
    "                valid_dats_neg.append(n)\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "    #Train data\n",
    "    train_dats_pos = dats_pos.copy()\n",
    "    train_dats_neg = dats_neg.copy()\n",
    "    for d in valid_dats_pos:\n",
    "        train_dats_pos.remove(d)\n",
    "    for n in valid_dats_neg:\n",
    "        train_dats_neg.remove(n)\n",
    "    \n",
    "    valid_pos_num = len([n for n in range(0, len(valid_dats_pos), shard_size)]) - 1\n",
    "    valid_neg_num = len([n for n in range(0, len(valid_dats_neg), shard_size)]) - 1\n",
    "    train_pos_num = len([n for n in range(0, len(train_dats_pos), shard_size)]) - 1\n",
    "    train_neg_num = len([n for n in range(0, len(train_dats_neg), shard_size)]) - 1\n",
    "    \n",
    "    if not glob.glob(f'{valid_path}/pos_shard_{valid_pos_num}/*/*/*.snapshot'): \n",
    "        \n",
    "        # process positive valid\n",
    "        process_data(\n",
    "            dats=valid_dats_pos, \n",
    "            cls=1, \n",
    "            shape=shape, \n",
    "            shard_size=shard_size, \n",
    "            path_root=valid_path, \n",
    "            path_modifier='pos_')\n",
    "            \n",
    "    elif not glob.glob(f'{valid_path}/neg_shard_{valid_neg_num}/*/*/*.snapshot'): \n",
    "        \n",
    "        # process positive valid\n",
    "        process_data(\n",
    "            dats=valid_dats_neg, \n",
    "            cls=0, \n",
    "            shape=shape, \n",
    "            shard_size=shard_size, \n",
    "            path_root=valid_path, \n",
    "            path_modifier='neg_')\n",
    "    \n",
    "    elif not glob.glob(f'{train_path}/pos/shard_{train_pos_num}/*/*/*.snapshot'):\n",
    "\n",
    "        # process positive train\n",
    "        process_data(\n",
    "            dats=train_dats_pos, \n",
    "            cls=1, \n",
    "            shape=shape, \n",
    "            shard_size=shard_size, \n",
    "            path_root=f'{train_path}/pos', \n",
    "            path_modifier='')\n",
    "    \n",
    "    elif not glob.glob(f'{train_path}/neg/shard_{train_neg_num}/*/*/*.snapshot'):\n",
    "        # process negative train\n",
    "        process_data(\n",
    "            dats=train_dats_neg, \n",
    "            cls=0, \n",
    "            shape=shape, \n",
    "            shard_size=shard_size, \n",
    "            path_root=f'{train_path}/neg', \n",
    "            path_modifier='')\n",
    "    \n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
