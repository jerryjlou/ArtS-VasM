{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHANGELOG\n",
    "\n",
    "v0.3 11/15/2023\n",
    "- hold out and external testing\n",
    "\n",
    "v0.2 10/17/2023\n",
    "- change to segmentation only\n",
    "\n",
    "v0.1 7/31/2023\n",
    "- OOM errors\n",
    "- process each patch individually and shard the dataset to save\n",
    "\n",
    "v0.0 7/30/2023\n",
    "- initiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[ 2023-11-17 14:09:44 ] WARNING 1 GPU device(s) requested but only 0 available \n"
     ]
    }
   ],
   "source": [
    "from jarvis.utils.general import gpus\n",
    "gpus.autoselect()\n",
    "\n",
    "import glob, numpy as np, pandas as pd, tensorflow as tf, matplotlib.pyplot as plt, os, time\n",
    "from jarvis.utils.display import imshow\n",
    "from jarvis.utils import arrays as jars\n",
    "from scipy import ndimage\n",
    "\n",
    "# import after setting OPENCV_IO_MAX_IMAGE_PIXELS to 2^50\n",
    "os.environ[\"OPENCV_IO_MAX_IMAGE_PIXELS\"] = pow(2,50).__str__() \n",
    "import cv2\n",
    "\n",
    "import sys  \n",
    "sys.path.append('/home/jjlou/Jerry/jerry_packages')\n",
    "from jerry_utils import restart_kernel, show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### recursive algorithm to create 784,784 patches at different resolution ####\n",
    "\n",
    "def create_tiles(arr, shape=(784, 784), **kwargs):\n",
    "\n",
    "    assert arr.ndim == 2\n",
    "\n",
    "    # --- Pad arr to square shape evenly divisible by shape / 2\n",
    "    ratio = np.ceil(np.array(arr.shape) / (np.array(shape) * 0.5))\n",
    "    full_ = np.round(ratio * (np.array(shape) * 0.5)).astype('int')\n",
    "    \n",
    "    arr = pad_zeros_to_shape(arr, shape=full_)\n",
    "\n",
    "    # --- Recursively create tiles\n",
    "    return create_tiles_recursive(raw=arr, shape=shape)\n",
    "\n",
    "def create_tiles_recursive(raw, shape=(784, 784), overlap=0.5, sub=None, tiles=None, **kwargs):\n",
    "\n",
    "    if tiles is None:\n",
    "        tiles = []\n",
    "\n",
    "    if sub is None:\n",
    "        sub = raw \n",
    "\n",
    "    # =================================================\n",
    "    # RECURSION | CASE 1 - SMALL ARRAY (FINISH)\n",
    "    # =================================================\n",
    "    if (sub.shape[0] <= shape[0]) and (sub.shape[1] <= shape[1]):\n",
    "        return tiles + [pad_zeros_to_shape(arr=sub, shape=shape)]\n",
    "\n",
    "    # =================================================\n",
    "    # RECURSION | CASE 2 - LARGE ARRAY\n",
    "    # =================================================\n",
    "\n",
    "    # --- Create tiles at current resolution\n",
    "    tiles += create_tiles_single(arr=sub, shape=shape, overlap=overlap, **kwargs)\n",
    "\n",
    "    # --- Subsample to lower resolution\n",
    "    res = (np.array(sub.shape) - np.array(shape) * overlap) / np.array(raw.shape)\n",
    "    sub = ndimage.zoom(raw, res)\n",
    "\n",
    "    return create_tiles_recursive(raw=raw, shape=shape, overlap=overlap, sub=sub, tiles=tiles) \n",
    "\n",
    "def create_tiles_single(arr, shape=(784, 784), overlap=0.5, **kwargs):\n",
    "\n",
    "    tiles = []\n",
    "\n",
    "    # --- Determine overlap step\n",
    "    ii, jj = np.round(np.array(shape) * overlap).astype('int')\n",
    "\n",
    "    for i in range(0, arr.shape[0] - shape[0] + 1, ii):\n",
    "        for j in range(0, arr.shape[1] - shape[1] + 1, jj):\n",
    "            tiles.append(arr[i:i+shape[0], j:j+shape[1]])\n",
    "\n",
    "    return tiles\n",
    "\n",
    "def pad_zeros_to_shape(arr, shape=(784, 784), **kwargs):\n",
    "\n",
    "    if (arr.shape[0] == shape[0]) and (arr.shape[1] == shape[1]):\n",
    "        return arr\n",
    "\n",
    "    assert arr.shape[0] <= shape[0]\n",
    "    assert arr.shape[1] <= shape[1]\n",
    "\n",
    "    lo = np.round((np.array(shape) - np.array(arr.shape)) / 2).astype('int')\n",
    "    hi = np.array(shape) - np.array(arr.shape) - lo\n",
    "\n",
    "    return np.pad(arr, ((lo[0], hi[0]), (lo[1], hi[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dat=None, msk=None, path_root=None, shard_size=None):\n",
    "    loaded_image = jars.create(dat).data[0].astype('uint8')\n",
    "    loaded_mask = jars.create(msk).data[0].astype('uint8')\n",
    "\n",
    "    ch0 = create_tiles(loaded_image[:,:,0])\n",
    "    ch1 = create_tiles(loaded_image[:,:,1])\n",
    "    ch2 = create_tiles(loaded_image[:,:,2])\n",
    "    channels = tf.data.Dataset.from_tensor_slices((ch0, ch1, ch2))\n",
    "    images = []\n",
    "    for zero, one, two in channels:\n",
    "        img = tf.stack([zero, one, two], axis=2)\n",
    "        images.append(img)\n",
    "\n",
    "    masks = create_tiles(loaded_mask[:,:,0])\n",
    "    \n",
    "    iterate = [n for n in range(0, len(images), shard_size)]\n",
    "    for i in iterate:\n",
    "        save_path = f'{path_root}/shard_{iterate.index(i)}'\n",
    "        if not glob.glob(f'{save_path}/*/*/*.snapshot'):\n",
    "            assert not glob.glob(f'{save_path}/*/*.shard'), f'{save_path} did not save properly'\n",
    "            shard_dats = images[i:i+shard_size]\n",
    "            shard_msks = masks[i:i+shard_size]\n",
    "            shard = tf.data.Dataset.from_tensor_slices((shard_dats, shard_msks))\n",
    "            shard.save(save_path)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(dats=None, msks=None, path_root=None, shard_size=None):\n",
    "    num_shards = len(dats)//shard_size + 1\n",
    "    for d, m in zip(dats, msks):\n",
    "        ID = d.split('/')[-2]\n",
    "        folder = f'{path_root}/{ID}'\n",
    "        if not glob.glob(f'{folder}/shard_{num_shards-1}/*/*/*.snapshot'):\n",
    "            assert not glob.glob(f'{folder}/shard_{num_shards-1}/*/*.shard'), f'{folder} did not save properly'\n",
    "            load_data(dat=d, msk=m, path_root=folder, shard_size=shard_size)\n",
    "            restart_kernel()\n",
    "            time.sleep(10)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/jjlou/Jerry/wsi-arterio/vessel_detection_and_rough_segmentation/data_test'\n",
    "shard_size = 100\n",
    "\n",
    "# Create hold out and valid lists of patient IDs\n",
    "hold_out_list = ['UCI-15-12-MF-HE', 'UCI-15-12-OCC-HE']\n",
    "\n",
    "external_list = ['V019_B1_HE', 'V019_B3_HE']\n",
    "\n",
    "# Slides not included in study but present in folder\n",
    "remove = ['UCI-15-12-OCC-AB', 'UCI-15-12-ST-AB', 'V019_B2_AB40', 'V019_B3_AB40']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dats = sorted(glob.glob('/data/raw/wsi_arterio/data/*/patches_full/*/dat.hdf5'))\n",
    "msk = sorted(glob.glob('/data/raw/wsi_arterio/data/*/patches_full/*/lbl.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extra folders that were excluded from study\n",
    "dats_remove = []\n",
    "msk_remove = []\n",
    "for r in remove:\n",
    "    for d in dats:\n",
    "        if d.find(r) != -1:\n",
    "            dats_remove.append(d)\n",
    "        else:\n",
    "            continue\n",
    "    for m in msk:\n",
    "        if m.find(r) != -1:\n",
    "            msk_remove.append(m)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "for dr in dats_remove:\n",
    "    dats.remove(dr)\n",
    "for mr in msk_remove:\n",
    "    msk.remove(mr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_out_dats_list = []\n",
    "hold_out_msk_list = []\n",
    "\n",
    "# Find all instances of the hold out patient IDs in dats and msk then save in corresponding list\n",
    "for h in hold_out_list:\n",
    "    for d in dats:\n",
    "        if d.find(h) != -1:\n",
    "            hold_out_dats_list.append(d)\n",
    "        else:\n",
    "            continue\n",
    "    for m in msk:\n",
    "        if m.find(h) != -1:\n",
    "            hold_out_msk_list.append(m)\n",
    "        else:\n",
    "            continue      \n",
    "\n",
    "hold_path = f'{root}/hold_out'\n",
    "if len(glob.glob(f'{hold_path}/*')) < len(hold_out_dats_list):\n",
    "    process_data(dats=hold_out_dats_list, msks=hold_out_msk_list, path_root=hold_path, shard_size=shard_size)        \n",
    "\n",
    "# Remove hold out dats and msks from original list\n",
    "for d in hold_out_dats_list:\n",
    "    dats.remove(d)\n",
    "for m in hold_out_msk_list:\n",
    "    msk.remove(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_dats_list = []\n",
    "external_msk_list = []\n",
    "\n",
    "# Find all instances of the hold out patient IDs in dats and msk then save in corresponding list\n",
    "for h in external_list:\n",
    "    for d in dats:\n",
    "        if d.find(h) != -1:\n",
    "            external_dats_list.append(d)\n",
    "        else:\n",
    "            continue\n",
    "    for m in msk:\n",
    "        if m.find(h) != -1:\n",
    "            external_msk_list.append(m)\n",
    "        else:\n",
    "            continue      \n",
    "\n",
    "external_path = f'{root}/external'\n",
    "if len(glob.glob(f'{external_path}/*')) < len(external_dats_list):\n",
    "    process_data(dats=external_dats_list, msks=external_msk_list, path_root=external_path, shard_size=shard_size)        \n",
    "\n",
    "# Remove hold out dats and msks from original list\n",
    "for d in external_dats_list:\n",
    "    dats.remove(d)\n",
    "for m in external_msk_list:\n",
    "    msk.remove(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = f'{root}/train'\n",
    "if len(glob.glob(f'{train_path}/*')) < len(dats):\n",
    "    process_data(dats=dats, msks=msk, path_root=train_path, shard_size=shard_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
